{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import ADASYN, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('./Dyt-desktop.csv', delimiter=';').columns.values[:-1]\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "    # Extract data points (features) as a NumPy array\n",
    "    X = data.iloc[:, :-1].values\n",
    "\n",
    "    # Extract labels as a NumPy array\n",
    "    y = data['Dyslexia'].values\n",
    "    y = np.where(y == 'Yes', 1, 0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def pre_process(data, labels):\n",
    "    X, y = data, labels\n",
    "\n",
    "    # Replace NaN values with 0s\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X[i][j] = np.nan_to_num(X[i][j])\n",
    "\n",
    "    # Encode 'Male' to 0 and 'Female' to 1\n",
    "    X[:, 0] = np.where(X[:, 0] == 'Male', 0, 1)\n",
    "\n",
    "    # Encode 'Yes' to 1 and 'No' to 0\n",
    "    X[:, 1] = np.where(X[:, 1] == 'Yes', 1, 0)\n",
    "    X[:, 2] = np.where(X[:, 2] == 'Yes', 1, 0)\n",
    "\n",
    "    # Perform Min-Max scaling for non-'Accuracy' columns\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    for i, feature in enumerate(features):\n",
    "        if not feature.startswith('Accuracy'):\n",
    "            column_values = X[:, i].astype(float).reshape(-1, 1)\n",
    "            X[:, i] = scaler.fit_transform(column_values).flatten()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def cross_validate(X, y, n_folds=10, threshold=0.5, seed=42, oversampling=None):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    accuracies = []\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    rocs = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        X_train, y_train = X[train], y[train]\n",
    "        X_test, y_test = X[test], y[test]\n",
    "\n",
    "        if oversampling == 'smote':\n",
    "            oversampler = SMOTE(random_state=seed)\n",
    "        if oversampling == 'adasyn':\n",
    "            oversampler = ADASYN(random_state=seed)\n",
    "        if oversampling is not None:\n",
    "            X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_data('./Dyt-desktop.csv')\n",
    "data, labels = pre_process(data, labels)\n",
    "X_train,y_train,X_test,y_test = cross_validate(data,labels,oversampling='adasyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.8240885\ttest: 0.7664835\tbest: 0.7664835 (0)\ttotal: 19.6ms\tremaining: 29.4s\n",
      "100:\tlearn: 0.9578365\ttest: 0.8928571\tbest: 0.9010989 (72)\ttotal: 2.19s\tremaining: 30.4s\n",
      "200:\tlearn: 0.9716606\ttest: 0.9175824\tbest: 0.9230769 (175)\ttotal: 4.22s\tremaining: 27.3s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9230769231\n",
      "bestIteration = 175\n",
      "\n",
      "Shrink model to first 176 iterations.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       321\n",
      "           1       0.49      0.74      0.59        43\n",
      "\n",
      "    accuracy                           0.88       364\n",
      "   macro avg       0.73      0.82      0.76       364\n",
      "weighted avg       0.91      0.88      0.89       364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1500,  # Number of trees\n",
    "    depth=6,  # Tree depth\n",
    "    learning_rate=0.03,  # Controls step size\n",
    "    loss_function='Logloss',  # For binary classification\n",
    "    eval_metric='Accuracy',\n",
    "    verbose=100,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "\n",
    "# Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate model performance\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class (\"Yes\")\n",
    "threshold = 0.3  # Adjust this based on precision-recall tuning\n",
    "y_pred_threshold = (y_prob >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_threshold))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best Parameters: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       321\n",
      "           1       0.49      0.74      0.59        43\n",
      "\n",
      "    accuracy                           0.88       364\n",
      "   macro avg       0.73      0.82      0.76       364\n",
      "weighted avg       0.91      0.88      0.89       364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "\n",
    "# Define a larger search space for hyperparameters\n",
    "param_grid = {\n",
    "    'C': [1, 10,100],  # More values for regularization\n",
    "    'gamma': [1, 0.1, 0.01],  # Wider range for RBF gamma\n",
    "    'kernel': ['rbf'],  # Keeping the best performing kernel\n",
    "}\n",
    "\n",
    "# Initialize SVC\n",
    "svm_model = SVC(probability=True)\n",
    "\n",
    "# Using RandomizedSearchCV for better hyperparameter tuning\n",
    "'''random_search = RandomizedSearchCV(\n",
    "    svm_model, param_distributions=param_grid,\n",
    "    n_iter=20, cv=10, verbose=2, random_state=42, n_jobs=-1\n",
    ")'''\n",
    "random_search = GridSearchCV(\n",
    "    svm_model, param_grid,\n",
    "    cv=5, verbose=2, n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Parameters\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "best_model = random_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# print(report)\n",
    "\n",
    "# def predict_data(info):\n",
    "#     print(best_model.predict(info))\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class (\"Yes\")\n",
    "threshold = 0.3  # Adjust this based on precision-recall tuning\n",
    "y_pred_threshold = (y_prob >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.91       321\n",
      "           1       0.41      0.77      0.54        43\n",
      "\n",
      "    accuracy                           0.84       364\n",
      "   macro avg       0.69      0.81      0.72       364\n",
      "weighted avg       0.90      0.84      0.86       364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, class_weight=None, random_state=42)\n",
    "model.fit(X_train,y_train)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class (\"Yes\")\n",
    "threshold = 0.3  # Adjust this based on precision-recall tuning\n",
    "y_pred_threshold = (y_prob >= threshold).astype(int)\n",
    "print(classification_report(y_test, y_pred_threshold))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
